{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNR0djHsl6IG10dbIEpkfnC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 카테고리별 뉴스 제목, 기사, 언론사 크롤링 코드   \n","\n","- 날짜 : 2022.09.01 ~ 2023.08.31   \n","- 카테고리 (예시) : <서울>   \n","- url 형식 : f\"https://news.daum.net/breakingnews/society/nation/seoul?page={page}&regDate={date}\"\n"],"metadata":{"id":"U5efX4X9ca93"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKb82JwWcVOA"},"outputs":[],"source":["from bs4 import BeautifulSoup as bs\n","import pandas as pd\n","import re\n","import requests\n","import pickle\n","from tqdm import tqdm"]},{"cell_type":"markdown","source":["### 📅 날짜 생성"],"metadata":{"id":"9OLX6870coec"}},{"cell_type":"code","source":["# 날짜 생성\n","dates = pd.date_range(\"2023-01-01\", \"2023-01-31\")\n","\n","# 날짜에서 하이픈(-) 제거\n","kdates = [re.sub('-', '', str(date)[0:10]) for date in dates]\n","print(kdates)"],"metadata":{"id":"hxyLsdnycnIh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 📰 카테고리별 뉴스 크롤링 (전체 페이지)\n","\n","- 주의사항) URL 안의 카테고리의 경우, 큰 카테고리도 있고 카테고리 안에 또 카테고리인 경우도 있어서 이 부분에 있어서는 직접 변경하는 것이 더 간편하다 생각하였음. 따라서 원하는 카테고리가 있다면 함수 내부의 url을 변경해야 함."],"metadata":{"id":"xH7edDKNcuXs"}},{"cell_type":"code","source":["def category_crawling(date):\n","    print('date =', date)\n","\n","    all_news = []\n","    current_page = 1\n","    last = True\n","\n","    while last == True:\n","\n","        try:\n","            # 10 페이지씩 크롤링\n","            for page in tqdm(range(current_page, current_page + 10)):\n","                # print(\"page =\", page)\n","                # 각 페이지에 접근\n","                url = f'https://news.daum.net/breakingnews/society/nation/seoul?page={page}&regDate={date}'\n","                res = requests.get(url)\n","                soup = bs(res.text, 'lxml')\n","                ul = soup.find(\"ul\", {\"class\": \"list_news2 list_allnews\"}).findAll(\"li\")\n","\n","\n","                for li in ul:\n","                    data = li.find(\"a\", {\"class\": \"link_txt\"})\n","                    press = li.find(\"span\", {\"class\": \"info_news\"}).text\n","\n","                    news_url = data.get(\"href\")\n","                    news_res = requests.get(news_url)\n","                    news_soup = bs(news_res.text, 'lxml')\n","                    article = news_soup.find(\"div\", {\"class\": \"article_view\"}).find(\"section\").findAll(\"p\")[:-1]\n","                    contents = \" \".join([p.text for p in article])\n","\n","                    all_news.append({\n","                        'title': data.text,\n","                        'url': news_url,\n","                        'press': press,\n","                        'content': contents\n","                    })\n","        except Exception as e:\n","            print('오류내용 :', e)\n","\n","        # \"다음\" 버튼이 있는지 확인\n","        try:\n","            next_button = soup.select_one('.btn_page.btn_next')\n","\n","            # \"다음\" 버튼이 없으면 종료\n","            if not next_button:\n","                #print(f'페이지 {current_page}부터 {current_page + 9}까지 크롤링 완료')\n","                last = False\n","                # break\n","            else:\n","                # 다음 10 페이지로 이동\n","                current_page += 10\n","\n","        except:\n","            print(\"페이지가 없음\")\n","\n","    return all_news"],"metadata":{"id":"A6BxWVCUcnK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 🏃 뉴스 크롤링"],"metadata":{"id":"po_N3I2GdJRf"}},{"cell_type":"code","source":["seoul_crawling = [category_crawling(date) for date in kdates]"],"metadata":{"id":"DSkwWMStcnM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ➡️ Dict -> DataFrame"],"metadata":{"id":"Cqz3M7cMdQu0"}},{"cell_type":"code","source":["df_news = []\n","for news_list in seoul_crawling:\n","    df_news.extend(news_list)\n","\n","news = pd.DataFrame(df_news)\n","\n","final_news = news.drop_duplicates(subset='url')\n","\n","print(f'중복제거된 기사 개수는 {len(news) - len(final_news)}입니다')"],"metadata":{"id":"AEZtMuFCcnO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 🥒 피클로 저장"],"metadata":{"id":"zWs8shbzdakB"}},{"cell_type":"code","source":["path = r'C:\\0_Semi_project\\data\\seoul'\n","\n","with open(path + '/seoul_20230101_20230131.pkl', mode='wb') as f:\n","    pickle.dump(final_news, f)"],"metadata":{"id":"iP3Ooa0kdaJ5"},"execution_count":null,"outputs":[]}]}